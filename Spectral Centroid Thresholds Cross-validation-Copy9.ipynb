{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MIR to help create a Wall of Sound Composition \n",
    "## Comparing Spectral Centroid / Zero Crossing Rate / Spectral Flatness\n",
    "\n",
    "### Description:\n",
    "\n",
    "Wall of Sound is a production and sound design technique based on layering sounds to create a fuller and thicker sound. You can find a deeper explanation of this technique <a href=\"https://en.wikipedia.org/wiki/Wall_of_Sound\">here</a>.\n",
    "\n",
    "I am creating a wall of sound with a collection of audio samples that I selected because I think that will sound good together. An extensive collection of samples was analyzed previously, and for this exercise, I choose the ones that are in the same tonality.\n",
    "\n",
    "When layering sounds, I have observed that there is the need of distributing the samples across the spectrum. For this I have created three separate frequency bands (low, mid and high frequencies) and I need to classify the samples to fit into this bands. This classification may create the problem of what is considered to belong to each band, because samples may contain energy in all frequencies. In this exercise I am trying to see where this sounds contribute the best so I can select the right amount of samples per band.\n",
    "\n",
    "### Objective:\n",
    "\n",
    "To distribute this sounds, I have found that audio features like Spectral Centroid, Zero Crossing Rate and Spectral Flatness can help in finding the sounds position in the spectrum. To make a complete analysis, I will compare this features to find the right crossover frequencies (low-mid crossover frequency and mid-high crossover frequency), based on ground truth I provide using Leave One Out Cross Validation model. \n",
    "\n",
    "This notebook helps to analyze a set of audio files, ther ground truth and cross validates to find the right low-mid crossover and mid-high crossover frequencies to distribute the samples across the spectrum.\n",
    "\n",
    "### Tools:\n",
    "\n",
    "For this task I will use Essentia library, which is a library for audio analysis and feature extraction, as well as Pandas, a library for Data Science to analyze the data. Pandas uses a data structure called Data Frame, and it stores, merges, analyzes and retrieves data in a way similar to a SQL Relational Database. This tool helps to visualize the steps and analyze this process. Often in this notebook I will display the data in this Data Frames to help understand the current status in the process.\n",
    "\n",
    "__Note:__ The term Crossover Frequency is used in Audio Engineering to refer to the set frequencies that divide the spectrum into a number of bands. In this classification problem it refers to the thresholds that separate classification of samples, even when I continue to refer to them as Crossover, both in the code and comments.\n",
    "\n",
    "Joaquin Jimenez Sauma (SMC17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import IPython\n",
    "import pandas as pd\n",
    "#import ipywidgets as widgets\n",
    "from essentia import *\n",
    "from essentia.standard import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth\n",
    "\n",
    "The next cell shows the classification I am using according to my personal opinion after listening each file. Sounds like pinknoise.wav, whitenoise.wav, sine20khz.wav, and sine50hz.wav are for testing purposes, and won't be used in the actual piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth = {\n",
    "    'pinknoise.wav': 'm'\n",
    "    ,'sine20khz.wav': 'h'\n",
    "    ,'sine50hz.wav': 'l'\n",
    "    ,'sound1.wav': 'm'\n",
    "    ,'sound2.wav': 'm'\n",
    "    ,'sound3.wav': 'm'\n",
    "    ,'sound4.wav': 'h'\n",
    "    ,'sound5.wav': 'l'\n",
    "    ,'sound6.wav': 'h'\n",
    "    ,'sound7.wav': 'l'\n",
    "    ,'sound8.wav': 'h'\n",
    "    ,'sound9.wav': 'm'\n",
    "    ,'sound10.wav': 'm'\n",
    "    ,'sound11.wav': 'h'\n",
    "    ,'sound12.wav': 'm'\n",
    "    ,'sound13.wav': 'l'\n",
    "    ,'sound14.wav': 'h'\n",
    "    ,'sound15.wav': 'l'\n",
    "    ,'whitenoise.wav': 'h'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next cell reads files in `path` and loads them to compute the spectral centroid, Zero Crossing Rate and Spectral Flatness\n",
    "\n",
    "Selected files are in the specified path in the next cell, where I am loading them into variables in memory, si I can extract and process its features. This code looks for files with `.wav` extension, but you can change it and use any kind of audio file format. \n",
    "\n",
    "This cell saves the stream of values for each feature in `scList`, `zcList` and `sfList`, where __sc__ stands for Spectral Centroid, __zc__ for Zero Crossing Rate and __sf__ for Spectral Flatness, I will use this prefixes from now on in this notebook.\n",
    "\n",
    "I use `data` Data Frame to compute and store the average of this features per file for future reference and operations.\n",
    "\n",
    "__Please update `path` variable to point to the folder that contains the audio files.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell reads files and extracts features. Features are saved in scList, zcList and sfList lists\n",
    "# Averages features per file and are saved on data (a Pandas Datagrid)\n",
    "\n",
    "path = '../../data/wavs/'\n",
    "\n",
    "row = {}\n",
    "data = pd.DataFrame(row, columns=('file', 'sc', 'zc', 'sf', 'gt'))\n",
    "data.set_index(['file'])\n",
    "w = Windowing()\n",
    "spec = Spectrum()\n",
    "centroid = Centroid(range=1) # Normalized to 1 so it can be compared to ZCR and SF\n",
    "\n",
    "scList = {} #spectral centroid\n",
    "zcList = {} #zero crossing rate\n",
    "sfList = {} #spectral flatness\n",
    "\n",
    "# Create data Datagrid to contain mean of each feature for each file\n",
    "# Keeps features by frame in arrays (scList, zcList and sfList) to plot\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".wav\"):\n",
    "        audio = MonoLoader(filename= path + file, sampleRate=44100)()\n",
    "        c = []\n",
    "        z = []\n",
    "        f = []\n",
    "        for frame in FrameGenerator(audio, frameSize = 1024, hopSize = 512):\n",
    "            c.append(centroid(spec(w(frame))))\n",
    "            z.append(ZeroCrossingRate()(frame))\n",
    "            f.append(Flatness()(spec(w(frame))))        \n",
    "        scList[file] = np.array(c)\n",
    "        zcList[file] = np.array(z)\n",
    "        sfList[file] = np.array(f)\n",
    "        row['file'] = file\n",
    "        row['sc'] = np.mean(scList[file][5:-5])\n",
    "        row['zc'] = np.mean(zcList[file][5:-5])\n",
    "        row['sf'] = np.mean(sfList[file][5:-5])\n",
    "        row['gt'] = groundTruth[file]\n",
    "        data.loc[len(data)] = (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the list of tests to be performed\n",
    "\n",
    "Next cell generates a list of the tests to be performed, and will be used in the process. \n",
    "\n",
    "Please note that this is a long process, it depends on the value of variable `step`. This defines the step size when generating crossovers. If you want to run the process faster, it is recommended to increase its size. Values can vary from 40 to 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a set of tests for each file\n",
    "\n",
    "step = 50\n",
    "files = data.sort_values(by=['file'])['file'].tolist()\n",
    "row={}\n",
    "tests = pd.DataFrame(row, columns = ['file', # Training file\n",
    "                                     'test', # Test file (only used until the end)\n",
    "                                     'lowCrossover', # Low Thresdhold value, the one that is left out\n",
    "                                     'highCrossover', # High Thresdhold value\n",
    "                                     'scClass', # Spectral Centroid class (can be 'l', 'm', 'h')\n",
    "                                     'zcClass', # Zero Crossing Rate class (can be 'l', 'm', 'h')\n",
    "                                     'sfClass']) # Spectral Flatness class (can be 'l', 'm', 'h')\n",
    "tests.set_index(['file', 'test', 'lowCrossover', 'highCrossover'])\n",
    "\n",
    "for file in files: \n",
    "    for leftOut in files: # Select the file that is left out, is shown here just for debugging, not being tested here\n",
    "        if file != leftOut:\n",
    "            for lowCrossover in range(5, 950, step): \n",
    "                for highCrossover in range (10, 1000, step):\n",
    "                    if lowCrossover < highCrossover:\n",
    "                        row['file'] = file\n",
    "                        row['test'] = leftOut\n",
    "                        row['lowCrossover'] = lowCrossover/1000\n",
    "                        row['highCrossover'] = highCrossover/1000\n",
    "                        row['scClass'] = ''\n",
    "                        row['zcClass'] = ''\n",
    "                        row['sfClass'] = ''\n",
    "                        tests.loc[len(tests)] = (row)   \n",
    "                        \n",
    "# Tests Dataframe with a combination of crossovers for each file\n",
    "tests.sort_values(\n",
    "    by=['test',\n",
    "        'file',\n",
    "        'lowCrossover', \n",
    "        'highCrossover' ])[\n",
    "    ['file', \n",
    "     'test',\n",
    "     'lowCrossover', \n",
    "     'highCrossover', \n",
    "     'scClass', \n",
    "     'zcClass', \n",
    "     'sfClass']]\n",
    "\n",
    "try:\n",
    "    os.remove(path + 'tests.csv')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# Save tests to csv file\n",
    "tests.to_csv(path + 'tests.csv')\n",
    "\n",
    "# Use provided data file or file created in previous cell\n",
    "tests = pd.read_csv(path + 'tests.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "Next cells process implement Leave One Out Cross Validation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tests.set_index(['file', 'test', 'lowCrossover', 'highCrossover'])\n",
    "\n",
    "# Merge tests dataframe with data dataframe to have ground truth and extracted features for each training file\n",
    "tests = pd.merge(tests,\n",
    "                   data,\n",
    "                   left_on='file',\n",
    "                   right_on='file',\n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classify each feature on each file for every set of crossovers\n",
    "for row in tests:\n",
    "    tests['scClass'] = np.where(tests.sc < tests.lowCrossover, 'l', \n",
    "        np.where(tests.sc > tests.highCrossover, 'h', 'm'))\n",
    "    tests['zcClass'] = np.where(tests.zc < tests.lowCrossover, 'l', \n",
    "        np.where(tests.zc > tests.highCrossover, 'h', 'm'))\n",
    "    tests['sfClass'] = np.where(tests.sf < tests.lowCrossover, 'l', \n",
    "        np.where(tests.sf > tests.highCrossover, 'h', 'm'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classify each feature on each file for every set of crossovers\n",
    "for test in tests:\n",
    "    tests['scResult'] = np.where(tests['scClass'] == tests['gt'], True, False)\n",
    "    tests['zcResult'] = np.where(tests['zcClass'] == tests['gt'], True, False)\n",
    "    tests['sfResult'] = np.where(tests['sfClass'] == tests['gt'], True, False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From dataframe we can start to compute our results\n",
    "\n",
    "# Computing learned crossovers \n",
    "\n",
    "learned = pd.merge(pd.merge(\n",
    "    tests.loc[tests['scResult'] == True].agg({\n",
    "    'lowCrossover': ['mean'],\n",
    "    'highCrossover': ['mean']\n",
    "    }), \n",
    "    tests.loc[tests['zcResult'] == True].agg({\n",
    "    'lowCrossover': ['mean'],\n",
    "    'highCrossover': ['mean']\n",
    "    }), \n",
    "    right_index=True, left_index=True, sort=False),\n",
    "    tests.loc[tests['sfResult'] == True].agg({\n",
    "    'lowCrossover': ['mean'],\n",
    "    'highCrossover': ['mean']\n",
    "    }),\n",
    "    right_index=True, left_index=True, sort=False)\n",
    "\n",
    "learned.rename(columns ={'highCrossover_x': 'scHC'}, inplace =True)\n",
    "learned.rename(columns ={'highCrossover_y': 'zcHC'}, inplace =True)\n",
    "learned.rename(columns ={'highCrossover': 'sfHC'}, inplace =True)\n",
    "learned.rename(columns ={'lowCrossover_x': 'scLC'}, inplace =True)\n",
    "learned.rename(columns ={'lowCrossover_y': 'zcLC'}, inplace =True)\n",
    "learned.rename(columns ={'lowCrossover': 'sfLC'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the error\n",
    "\n",
    "# Total number of tests\n",
    "total = len(tests)\n",
    "\n",
    "error = pd.merge(pd.merge(\n",
    "    tests.loc[tests['scResult'] == True].agg({\n",
    "    'file': ['count'] \n",
    "    }), \n",
    "    tests.loc[tests['zcResult'] == True].agg({\n",
    "    'file': ['count']\n",
    "    }), \n",
    "    right_index=True, left_index=True, sort=False),\n",
    "    tests.loc[tests['sfResult'] == True].agg({\n",
    "    'file': ['count']\n",
    "    }),\n",
    "    right_index=True, left_index=True, sort=False)\n",
    "\n",
    "error.rename(columns ={'file_x': 'SC'}, inplace =True)\n",
    "error.rename(columns ={'file_y': 'ZC'}, inplace =True)\n",
    "error.rename(columns ={'file': 'SF'}, inplace =True)\n",
    "\n",
    "error['SC'] = error['SC'].iloc[0] / total\n",
    "error['ZC'] = error['ZC'].iloc[0] / total\n",
    "error['SF'] = error['SF'].iloc[0] / total\n",
    "error.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a new Dataframe to show learned crossovers\n",
    "results = pd.merge(pd.merge(\n",
    "    tests.loc[tests['scResult'] == True].groupby(['test']).agg({\n",
    "    'lowCrossover': ['mean'],\n",
    "    'highCrossover': ['mean'],\n",
    "    'file': ['count']\n",
    "    }), \n",
    "    tests.loc[tests['zcResult'] == True].groupby(['test']).agg({\n",
    "    'lowCrossover': ['mean'],\n",
    "    'highCrossover': ['mean'],\n",
    "    'file': ['count']\n",
    "    }), \n",
    "    right_index=True, left_index=True, sort=False),\n",
    "    tests.loc[tests['sfResult'] == True].groupby(['test']).agg({\n",
    "    'lowCrossover': ['mean'],\n",
    "    'highCrossover': ['mean'],\n",
    "    'file': ['count']\n",
    "    }),\n",
    "    right_index=True, left_index=True, sort=False)\n",
    "\n",
    "# Rename columns\n",
    "results = pd.concat([results, tests.groupby(['test']).size().to_frame()], axis = 1)\n",
    "results.rename(columns ={0: 'Total'}, inplace =True)\n",
    "results.rename(columns ={('highCrossover_x', 'mean'): 'scHC'}, inplace =True)\n",
    "results.rename(columns ={('highCrossover_y', 'mean'): 'zcHC'}, inplace =True)\n",
    "results.rename(columns ={('highCrossover', 'mean'): 'sfHC'}, inplace =True)\n",
    "results.rename(columns ={('lowCrossover_x', 'mean'): 'scLC'}, inplace =True)\n",
    "results.rename(columns ={('lowCrossover_y', 'mean'): 'zcLC'}, inplace =True)\n",
    "results.rename(columns ={('lowCrossover', 'mean'): 'sfLC'}, inplace =True)\n",
    "results.rename(columns ={('file_x', 'count'): 'scTotal'}, inplace =True)\n",
    "results.rename(columns ={('file_y', 'count'): 'zcTotal'}, inplace =True)\n",
    "results.rename(columns ={('file', 'count'): 'sfTotal'}, inplace =True)\n",
    "results.index.name = 'test'\n",
    "results.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence level for each feature, for each file\n",
    "\n",
    "for row in results:\n",
    "    results['scConfidence'] = results['scTotal'] / results['Total']\n",
    "    results['zcConfidence'] = results['zcTotal'] / results['Total']\n",
    "    results['sfConfidence'] = results['sfTotal'] / results['Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results dataframe with data dataframe to have ground truth and extracted features for each training file\n",
    "# First I'll put the indexes in order\n",
    "results = pd.merge(results,\n",
    "                   data,\n",
    "                   left_on='test',\n",
    "                   right_on='file',\n",
    "                   how='left')\n",
    "results = results.copy()\n",
    "\n",
    "# The merge step put all fields together, so I will select the ones I want to keep\n",
    "results = results [[\n",
    "    'test',\n",
    "    'gt',\n",
    "    'sc',\n",
    "    'zc',\n",
    "    'sf',\n",
    "    'scLC',\n",
    "    'scHC',\n",
    "    'scTotal',\n",
    "    'zcLC',\n",
    "    'zcHC',\n",
    "    'zcTotal',\n",
    "    'sfLC',\n",
    "    'sfHC',\n",
    "    'sfTotal',\n",
    "    'Total',\n",
    "    'scConfidence',\n",
    "    'zcConfidence',\n",
    "    'sfConfidence'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def testValue(gt, value, lc, hc):\n",
    "    if value < lc:\n",
    "        r = 'l'\n",
    "    elif value > hc:\n",
    "        r = 'h'\n",
    "    else:\n",
    "        r = 'm'\n",
    "    \n",
    "    if r == gt:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Test left out file on learned crossovers\n",
    "for row in results:\n",
    "    results['scPassed'] = testValue(results['gt'].iloc[0], results['sc'].iloc[0], results['scLC'].iloc[0], results['scHC'].iloc[0])\n",
    "    results['zcPassed'] = testValue(results['gt'].iloc[0], results['zc'].iloc[0], results['zcLC'].iloc[0], results['zcHC'].iloc[0])\n",
    "    results['sfPassed'] = testValue(results['gt'].iloc[0], results['sf'].iloc[0], results['sfLC'].iloc[0], results['sfHC'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Next cell prints the results of the evaluation. The crossovers learned, error per feature and the list of tests for each iteration of the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned crossovers:\n",
      "          scLC      scHC      zcLC      zcHC      sfLC      sfHC\n",
      "mean  0.267033  0.604337  0.277296  0.608709  0.264533  0.611169\n",
      "\n",
      "Error per feature:\n",
      "         SC        ZC        SF\n",
      "0  0.391337  0.360866  0.409469\n"
     ]
    }
   ],
   "source": [
    "print('Learned crossovers:')\n",
    "print(learned[['scLC', 'scHC', 'zcLC', 'zcHC', 'sfLC', 'sfHC']])\n",
    "print('')\n",
    "print('Error per feature:')\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Leave One Out Cross Validation seems accurate to find crossover frequencies. When testing using more values (setting smaller values for `step` variable), error numbers are smaller. This may indicate that with more detail, we can find better values. This is a lenghty process and requires a lot of computational power, but with more time I would like to experiment with even more detail. Anonther good experiment would be to use more samples to see how much this method can be improved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
